{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "d3WhoYlxxc67",
    "colab_type": "code",
    "outputId": "b408d0f6-2438-45b6-b44f-b85c2a2bd1ce",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.560693905748E12,
     "user_tz": -540.0,
     "elapsed": 949.0,
     "user": {
      "displayName": "L Kan",
      "photoUrl": "",
      "userId": "05643648821851919258"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Author: Inyong Hwang (lkan6004@gmail.com)\n",
    "# Date: 2019-06-17-Mon\n",
    "# Korean Character STR 20*20 by ResNet epoch=1~100 \n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SdC6ExhCxkKN",
    "colab_type": "code",
    "outputId": "4079c966-632d-4a28-9a13-a4e17418a778",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.560693913031E12,
     "user_tz": -540.0,
     "elapsed": 4751.0,
     "user": {
      "displayName": "L Kan",
      "photoUrl": "",
      "userId": "05643648821851919258"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Google Drive=====\n",
      " 논문\t\t\t      'Colab Notebooks'   Lab-Desktop   PUBLIC\n",
      "'AI 사물인식 해커톤 (2).zip'   Dataset\t\t  Program       USB\n",
      "=====input=====\n",
      "images.pkl  labels.pkl\n"
     ]
    }
   ],
   "source": [
    "print('=====Google Drive=====')\n",
    "!ls '/content/drive/My Drive/'\n",
    "print('=====input=====')\n",
    "!ls '/content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/PKL_20v4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zajE3nISxsQA",
    "colab_type": "code",
    "outputId": "4915e17b-8810-47ff-88fe-1522a264698a",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.56070074723E12,
     "user_tz": -540.0,
     "elapsed": 6660578.0,
     "user": {
      "displayName": "L Kan",
      "photoUrl": "",
      "userId": "05643648821851919258"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 16875.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading labels.pkl\n",
      "loaded!\n",
      "loading images.pkl\n",
      "loaded!\n",
      "# of images: 139104\n",
      "Shape of images: (20, 20, 1)\n",
      "# of train set: 111283 # of test set: 27821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 14:08:13.837838 139690822580096 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input_Image (InputLayer)        (None, 20, 20, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 20, 20, 16)   160         Input_Image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 20, 20, 16)   64          conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 20, 20, 16)   0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 20, 20, 16)   272         activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 20, 20, 16)   64          conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 20, 20, 16)   0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 20, 20, 16)   2320        activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 20, 20, 16)   64          conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 20, 20, 16)   0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 20, 20, 64)   1088        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 20, 20, 64)   1088        activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 20, 20, 64)   0           conv2d_63[0][0]                  \n",
      "                                                                 conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 20, 20, 64)   256         add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 20, 20, 64)   0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 20, 20, 16)   1040        activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 20, 20, 16)   64          conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 20, 20, 16)   0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 20, 20, 16)   2320        activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 20, 20, 16)   64          conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 20, 20, 16)   0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 20, 20, 64)   1088        activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 20, 20, 64)   0           add_19[0][0]                     \n",
      "                                                                 conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 20, 20, 64)   256         add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 20, 20, 64)   0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 20, 20, 16)   1040        activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 20, 20, 16)   64          conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 20, 20, 16)   0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 20, 20, 16)   2320        activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 20, 20, 16)   64          conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 20, 20, 16)   0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 20, 20, 64)   1088        activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 20, 20, 64)   0           add_20[0][0]                     \n",
      "                                                                 conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 20, 20, 64)   256         add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 20, 20, 64)   0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 20, 20, 16)   1040        activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 20, 20, 16)   64          conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 20, 20, 16)   0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 20, 20, 16)   2320        activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 20, 20, 16)   64          conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 20, 20, 16)   0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 20, 20, 64)   1088        activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 20, 20, 64)   0           add_21[0][0]                     \n",
      "                                                                 conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 20, 20, 64)   256         add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 20, 20, 64)   0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 20, 20, 16)   1040        activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 20, 20, 16)   64          conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 20, 20, 16)   0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 20, 20, 16)   2320        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 20, 20, 16)   64          conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 20, 20, 16)   0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 20, 20, 64)   1088        activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 20, 20, 64)   0           add_22[0][0]                     \n",
      "                                                                 conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 20, 20, 64)   256         add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 20, 20, 64)   0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 20, 20, 16)   1040        activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 20, 20, 16)   64          conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 20, 20, 16)   0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 20, 20, 16)   2320        activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 20, 20, 16)   64          conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 20, 20, 16)   0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 20, 20, 64)   1088        activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 20, 20, 64)   0           add_23[0][0]                     \n",
      "                                                                 conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 20, 20, 64)   256         add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 20, 20, 64)   0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 10, 10, 64)   4160        activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 10, 10, 64)   256         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 10, 10, 64)   0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 10, 10, 64)   36928       activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 10, 10, 64)   256         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 10, 10, 64)   0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 10, 10, 128)  8320        add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 10, 10, 128)  8320        activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 10, 10, 128)  0           conv2d_82[0][0]                  \n",
      "                                                                 conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 10, 10, 128)  512         add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 10, 10, 128)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 10, 10, 64)   8256        activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 10, 10, 64)   256         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 10, 10, 64)   0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 10, 10, 64)   36928       activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 10, 10, 64)   256         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 10, 10, 64)   0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 10, 10, 128)  8320        activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 10, 10, 128)  0           add_25[0][0]                     \n",
      "                                                                 conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 10, 10, 128)  512         add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 10, 10, 128)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 10, 10, 64)   8256        activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 10, 10, 64)   256         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 10, 10, 64)   0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 10, 10, 64)   36928       activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 10, 10, 64)   256         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 10, 10, 64)   0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 10, 10, 128)  8320        activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 10, 10, 128)  0           add_26[0][0]                     \n",
      "                                                                 conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 10, 10, 128)  512         add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 10, 10, 128)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 10, 10, 64)   8256        activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 10, 10, 64)   256         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 10, 10, 64)   0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 10, 10, 64)   36928       activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 10, 10, 64)   256         conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 10, 10, 64)   0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 10, 10, 128)  8320        activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 10, 10, 128)  0           add_27[0][0]                     \n",
      "                                                                 conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 10, 10, 128)  512         add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 10, 10, 128)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 10, 10, 64)   8256        activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 10, 10, 64)   256         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 10, 10, 64)   0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 10, 10, 64)   36928       activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 10, 10, 64)   256         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 10, 10, 64)   0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 10, 10, 128)  8320        activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 10, 10, 128)  0           add_28[0][0]                     \n",
      "                                                                 conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 10, 10, 128)  512         add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 10, 10, 128)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 10, 10, 64)   8256        activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 10, 10, 64)   256         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 10, 10, 64)   0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 10, 10, 64)   36928       activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 10, 10, 64)   256         conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 10, 10, 64)   0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 10, 10, 128)  8320        activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 10, 10, 128)  0           add_29[0][0]                     \n",
      "                                                                 conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 10, 10, 128)  512         add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 10, 10, 128)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 5, 5, 128)    16512       activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 5, 5, 128)    512         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 5, 5, 128)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 5, 5, 128)    147584      activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 5, 5, 128)    512         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 5, 5, 128)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 5, 5, 256)    33024       add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 5, 5, 256)    33024       activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 5, 5, 256)    0           conv2d_101[0][0]                 \n",
      "                                                                 conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 5, 5, 256)    1024        add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 5, 5, 256)    0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 5, 5, 128)    32896       activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 5, 5, 128)    512         conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 5, 5, 128)    0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 5, 5, 128)    147584      activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 5, 5, 128)    512         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 5, 5, 128)    0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 5, 5, 256)    33024       activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 5, 5, 256)    0           add_31[0][0]                     \n",
      "                                                                 conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 5, 5, 256)    1024        add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 5, 5, 256)    0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 5, 5, 128)    32896       activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 5, 5, 128)    512         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 5, 5, 128)    0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 5, 5, 128)    147584      activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 5, 5, 128)    512         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 5, 5, 128)    0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 5, 5, 256)    33024       activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 5, 5, 256)    0           add_32[0][0]                     \n",
      "                                                                 conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 5, 5, 256)    1024        add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 5, 5, 256)    0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 5, 5, 128)    32896       activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 5, 5, 128)    512         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 5, 5, 128)    0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 5, 5, 128)    147584      activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 5, 5, 128)    512         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 5, 5, 128)    0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 5, 5, 256)    33024       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 5, 5, 256)    0           add_33[0][0]                     \n",
      "                                                                 conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 5, 5, 256)    1024        add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 5, 5, 256)    0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 5, 5, 128)    32896       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 5, 5, 128)    512         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 5, 5, 128)    0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 5, 5, 128)    147584      activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 5, 5, 128)    512         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 5, 5, 128)    0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 5, 5, 256)    33024       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 5, 5, 256)    0           add_34[0][0]                     \n",
      "                                                                 conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 5, 5, 256)    1024        add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 5, 5, 256)    0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 5, 5, 128)    32896       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 5, 5, 128)    512         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 5, 5, 128)    0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 5, 5, 128)    147584      activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 5, 5, 128)    512         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 5, 5, 128)    0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 5, 5, 256)    33024       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 5, 5, 256)    0           add_35[0][0]                     \n",
      "                                                                 conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 5, 5, 256)    1024        add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 5, 5, 256)    0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 6400)         0           activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1008)         6452208     flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 8,123,088\n",
      "Trainable params: 8,112,688\n",
      "Non-trainable params: 10,400\n",
      "__________________________________________________________________________________________________\n",
      "Learning rate:  0.001\n",
      "ResNet56v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 14:08:14.990768 139690822580096 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 89026 samples, validate on 22257 samples\n",
      "Epoch 1/100\n",
      "Learning rate:  0.001\n",
      " - 87s - loss: 3.7707 - acc: 0.4649 - val_loss: 2.1285 - val_acc: 0.6794\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.67943, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_20/checkpoint.001.hg\n",
      "Epoch 2/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 1.3721 - acc: 0.8361 - val_loss: 1.8390 - val_acc: 0.7133\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.67943 to 0.71335, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_20/checkpoint.002.hg\n",
      "Epoch 3/100\n",
      "Learning rate:  0.001\n",
      " - 67s - loss: 0.9241 - acc: 0.9084 - val_loss: 1.2198 - val_acc: 0.8286\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.71335 to 0.82859, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_20/checkpoint.003.hg\n",
      "Epoch 4/100\n",
      "Learning rate:  0.001\n",
      " - 67s - loss: 0.7220 - acc: 0.9361 - val_loss: 1.7665 - val_acc: 0.7157\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.82859\n",
      "Epoch 5/100\n",
      "Learning rate:  0.001\n",
      " - 67s - loss: 0.6006 - acc: 0.9514 - val_loss: 1.0176 - val_acc: 0.8493\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.82859 to 0.84926, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_20/checkpoint.005.hg\n",
      "Epoch 6/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.5322 - acc: 0.9566 - val_loss: 1.2717 - val_acc: 0.7865\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.84926\n",
      "Epoch 7/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.4726 - acc: 0.9644 - val_loss: 0.9356 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.84926 to 0.85074, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_20/checkpoint.007.hg\n",
      "Epoch 8/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.4299 - acc: 0.9674 - val_loss: 1.0026 - val_acc: 0.8413\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.85074\n",
      "Epoch 9/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.3979 - acc: 0.9704 - val_loss: 0.9448 - val_acc: 0.8520\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.85074 to 0.85200, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_20/checkpoint.009.hg\n",
      "Epoch 10/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.3653 - acc: 0.9728 - val_loss: 2.6937 - val_acc: 0.6385\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.85200\n",
      "Epoch 11/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.3452 - acc: 0.9746 - val_loss: 0.9114 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.85200 to 0.86530, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_20/checkpoint.011.hg\n",
      "Epoch 12/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.3198 - acc: 0.9767 - val_loss: 0.8366 - val_acc: 0.8610\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.86530\n",
      "Epoch 13/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.3004 - acc: 0.9785 - val_loss: 0.8769 - val_acc: 0.8550\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.86530\n",
      "Epoch 14/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.2822 - acc: 0.9800 - val_loss: 0.8280 - val_acc: 0.8596\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.86530\n",
      "Epoch 15/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.2599 - acc: 0.9830 - val_loss: 0.7460 - val_acc: 0.8765\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.86530 to 0.87653, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_20/checkpoint.015.hg\n",
      "Epoch 16/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.2557 - acc: 0.9812 - val_loss: 3.8663 - val_acc: 0.5077\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.87653\n",
      "Epoch 17/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.2390 - acc: 0.9836 - val_loss: 1.3605 - val_acc: 0.7656\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.87653\n",
      "Epoch 18/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.2303 - acc: 0.9830 - val_loss: 0.8062 - val_acc: 0.8697\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.87653\n",
      "Epoch 19/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.2206 - acc: 0.9843 - val_loss: 0.7124 - val_acc: 0.8838\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.87653 to 0.88381, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_20/checkpoint.019.hg\n",
      "Epoch 20/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.2092 - acc: 0.9852 - val_loss: 0.9800 - val_acc: 0.8365\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.88381\n",
      "Epoch 21/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.2046 - acc: 0.9856 - val_loss: 3.2491 - val_acc: 0.5657\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.88381\n",
      "Epoch 22/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1951 - acc: 0.9860 - val_loss: 2.1718 - val_acc: 0.6945\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.88381\n",
      "Epoch 23/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.1887 - acc: 0.9867 - val_loss: 2.2277 - val_acc: 0.6903\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.88381\n",
      "Epoch 24/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.1805 - acc: 0.9875 - val_loss: 0.8874 - val_acc: 0.8565\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.88381\n",
      "Epoch 25/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1767 - acc: 0.9873 - val_loss: 0.8987 - val_acc: 0.8494\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.88381\n",
      "Epoch 26/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1692 - acc: 0.9878 - val_loss: 0.7402 - val_acc: 0.8737\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.88381\n",
      "Epoch 27/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1611 - acc: 0.9893 - val_loss: 0.9163 - val_acc: 0.8436\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.88381\n",
      "Epoch 28/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1605 - acc: 0.9878 - val_loss: 0.7758 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.88381\n",
      "Epoch 29/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1556 - acc: 0.9883 - val_loss: 0.7831 - val_acc: 0.8678\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.88381\n",
      "Epoch 30/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1470 - acc: 0.9898 - val_loss: 1.4461 - val_acc: 0.7709\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.88381\n",
      "Epoch 31/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1468 - acc: 0.9891 - val_loss: 2.0051 - val_acc: 0.6964\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.88381\n",
      "Epoch 32/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.1425 - acc: 0.9901 - val_loss: 0.7664 - val_acc: 0.8729\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.88381\n",
      "Epoch 33/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1370 - acc: 0.9901 - val_loss: 2.9921 - val_acc: 0.6329\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88381\n",
      "Epoch 34/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.1388 - acc: 0.9896 - val_loss: 0.7977 - val_acc: 0.8716\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88381\n",
      "Epoch 35/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.1315 - acc: 0.9907 - val_loss: 0.8800 - val_acc: 0.8629\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88381\n",
      "Epoch 36/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1262 - acc: 0.9914 - val_loss: 0.8298 - val_acc: 0.8554\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.88381\n",
      "Epoch 37/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.1264 - acc: 0.9904 - val_loss: 2.4471 - val_acc: 0.6715\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.88381\n",
      "Epoch 38/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1225 - acc: 0.9920 - val_loss: 1.3108 - val_acc: 0.8077\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.88381\n",
      "Epoch 39/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1249 - acc: 0.9902 - val_loss: 0.7923 - val_acc: 0.8759\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88381\n",
      "Epoch 40/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1190 - acc: 0.9918 - val_loss: 1.0819 - val_acc: 0.8325\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88381\n",
      "Epoch 41/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1166 - acc: 0.9918 - val_loss: 4.5680 - val_acc: 0.4869\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88381\n",
      "Epoch 42/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1131 - acc: 0.9923 - val_loss: 1.0076 - val_acc: 0.8427\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88381\n",
      "Epoch 43/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1100 - acc: 0.9922 - val_loss: 2.1220 - val_acc: 0.7161\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.88381\n",
      "Epoch 44/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1106 - acc: 0.9919 - val_loss: 0.7628 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.88381\n",
      "Epoch 45/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1063 - acc: 0.9925 - val_loss: 0.8520 - val_acc: 0.8623\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.88381\n",
      "Epoch 46/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1052 - acc: 0.9926 - val_loss: 0.7780 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.88381\n",
      "Epoch 47/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1036 - acc: 0.9929 - val_loss: 3.1292 - val_acc: 0.6043\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.88381\n",
      "Epoch 48/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1019 - acc: 0.9922 - val_loss: 1.2739 - val_acc: 0.8151\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.88381\n",
      "Epoch 49/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.1022 - acc: 0.9927 - val_loss: 6.7654 - val_acc: 0.3439\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.88381\n",
      "Epoch 50/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0955 - acc: 0.9935 - val_loss: 1.9087 - val_acc: 0.7348\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.88381\n",
      "Epoch 51/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0989 - acc: 0.9922 - val_loss: 7.0483 - val_acc: 0.3221\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.88381\n",
      "Epoch 52/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0953 - acc: 0.9933 - val_loss: 0.8538 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.88381\n",
      "Epoch 53/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0927 - acc: 0.9935 - val_loss: 4.9465 - val_acc: 0.4721\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.88381\n",
      "Epoch 54/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0910 - acc: 0.9936 - val_loss: 0.7513 - val_acc: 0.8817\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.88381\n",
      "Epoch 55/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0891 - acc: 0.9939 - val_loss: 6.1749 - val_acc: 0.3619\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.88381\n",
      "Epoch 56/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0879 - acc: 0.9940 - val_loss: 5.3869 - val_acc: 0.4137\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.88381\n",
      "Epoch 57/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0905 - acc: 0.9927 - val_loss: 0.9827 - val_acc: 0.8572\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.88381\n",
      "Epoch 58/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0891 - acc: 0.9937 - val_loss: 4.8997 - val_acc: 0.4814\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.88381\n",
      "Epoch 59/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0821 - acc: 0.9949 - val_loss: 1.9167 - val_acc: 0.7257\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.88381\n",
      "Epoch 60/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0884 - acc: 0.9930 - val_loss: 0.8161 - val_acc: 0.8781\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.88381\n",
      "Epoch 61/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0849 - acc: 0.9941 - val_loss: 4.8834 - val_acc: 0.4384\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.88381\n",
      "Epoch 62/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0784 - acc: 0.9952 - val_loss: 0.9460 - val_acc: 0.8576\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.88381\n",
      "Epoch 63/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0846 - acc: 0.9933 - val_loss: 0.8354 - val_acc: 0.8721\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.88381\n",
      "Epoch 64/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0804 - acc: 0.9940 - val_loss: 0.8924 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.88381\n",
      "Epoch 65/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0785 - acc: 0.9946 - val_loss: 0.7634 - val_acc: 0.8855\n",
      "\n",
      "Epoch 00065: val_acc improved from 0.88381 to 0.88547, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_20/checkpoint.065.hg\n",
      "Epoch 66/100\n",
      "Learning rate:  0.001\n",
      " - 66s - loss: 0.0821 - acc: 0.9936 - val_loss: 0.8832 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.88547\n",
      "Epoch 67/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0765 - acc: 0.9951 - val_loss: 1.5903 - val_acc: 0.7815\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.88547\n",
      "Epoch 68/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0771 - acc: 0.9945 - val_loss: 1.0100 - val_acc: 0.8425\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.88547\n",
      "Epoch 69/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0771 - acc: 0.9943 - val_loss: 0.7855 - val_acc: 0.8843\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.88547\n",
      "Epoch 70/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0773 - acc: 0.9941 - val_loss: 0.7632 - val_acc: 0.8788\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.88547\n",
      "Epoch 71/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0754 - acc: 0.9945 - val_loss: 1.4372 - val_acc: 0.7991\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.88547\n",
      "Epoch 72/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0713 - acc: 0.9954 - val_loss: 1.7833 - val_acc: 0.7211\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.88547\n",
      "Epoch 73/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0738 - acc: 0.9947 - val_loss: 1.0272 - val_acc: 0.8334\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.88547\n",
      "Epoch 74/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0709 - acc: 0.9950 - val_loss: 3.4978 - val_acc: 0.5915\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.88547\n",
      "Epoch 75/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0700 - acc: 0.9952 - val_loss: 0.8696 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.88547\n",
      "Epoch 76/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0684 - acc: 0.9954 - val_loss: 0.8459 - val_acc: 0.8687\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.88547\n",
      "Epoch 77/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0714 - acc: 0.9944 - val_loss: 1.1762 - val_acc: 0.8254\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.88547\n",
      "Epoch 78/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0715 - acc: 0.9944 - val_loss: 0.8097 - val_acc: 0.8797\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.88547\n",
      "Epoch 79/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0685 - acc: 0.9954 - val_loss: 2.8180 - val_acc: 0.6532\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.88547\n",
      "Epoch 80/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0677 - acc: 0.9953 - val_loss: 2.7071 - val_acc: 0.6561\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.88547\n",
      "Epoch 81/100\n",
      "Learning rate:  0.001\n",
      " - 65s - loss: 0.0669 - acc: 0.9950 - val_loss: 1.7193 - val_acc: 0.7599\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.88547\n",
      "Epoch 82/100\n",
      "Learning rate:  0.0001\n",
      " - 65s - loss: 0.0576 - acc: 0.9979 - val_loss: 0.6978 - val_acc: 0.8963\n",
      "\n",
      "Epoch 00082: val_acc improved from 0.88547 to 0.89635, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_20/checkpoint.082.hg\n",
      "Epoch 83/100\n",
      "Learning rate:  0.0001\n",
      " - 65s - loss: 0.0508 - acc: 0.9994 - val_loss: 0.6827 - val_acc: 0.8982\n",
      "\n",
      "Epoch 00083: val_acc improved from 0.89635 to 0.89819, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_20/checkpoint.083.hg\n",
      "Epoch 84/100\n",
      "Learning rate:  0.0001\n",
      " - 65s - loss: 0.0489 - acc: 0.9995 - val_loss: 0.6804 - val_acc: 0.8997\n",
      "\n",
      "Epoch 00084: val_acc improved from 0.89819 to 0.89972, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_20/checkpoint.084.hg\n",
      "Epoch 85/100\n",
      "Learning rate:  0.0001\n",
      " - 66s - loss: 0.0472 - acc: 0.9995 - val_loss: 0.6760 - val_acc: 0.9001\n",
      "\n",
      "Epoch 00085: val_acc improved from 0.89972 to 0.90008, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_20/checkpoint.085.hg\n",
      "Epoch 86/100\n",
      "Learning rate:  0.0001\n",
      " - 66s - loss: 0.0453 - acc: 0.9996 - val_loss: 0.6710 - val_acc: 0.9012\n",
      "\n",
      "Epoch 00086: val_acc improved from 0.90008 to 0.90115, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_20/checkpoint.086.hg\n",
      "Epoch 87/100\n",
      "Learning rate:  0.0001\n",
      " - 66s - loss: 0.0429 - acc: 0.9995 - val_loss: 0.6775 - val_acc: 0.8990\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.90115\n",
      "Epoch 88/100\n",
      "Learning rate:  0.0001\n",
      " - 65s - loss: 0.0406 - acc: 0.9995 - val_loss: 0.7301 - val_acc: 0.8916\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.90115\n",
      "Epoch 89/100\n",
      "Learning rate:  0.0001\n",
      " - 65s - loss: 0.0383 - acc: 0.9995 - val_loss: 0.6813 - val_acc: 0.8989\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.90115\n",
      "Epoch 90/100\n",
      "Learning rate:  0.0001\n",
      " - 65s - loss: 0.0357 - acc: 0.9996 - val_loss: 0.6664 - val_acc: 0.8986\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.90115\n",
      "Epoch 91/100\n",
      "Learning rate:  0.0001\n",
      " - 66s - loss: 0.0335 - acc: 0.9995 - val_loss: 0.6728 - val_acc: 0.8997\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.90115\n",
      "Epoch 92/100\n",
      "Learning rate:  0.0001\n",
      " - 65s - loss: 0.0316 - acc: 0.9995 - val_loss: 0.6261 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00092: val_acc improved from 0.90115 to 0.90304, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_20/checkpoint.092.hg\n",
      "Epoch 93/100\n",
      "Learning rate:  0.0001\n",
      " - 66s - loss: 0.0296 - acc: 0.9995 - val_loss: 0.7320 - val_acc: 0.8891\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.90304\n",
      "Epoch 94/100\n",
      "Learning rate:  0.0001\n",
      " - 66s - loss: 0.0278 - acc: 0.9996 - val_loss: 1.2172 - val_acc: 0.8213\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.90304\n",
      "Epoch 95/100\n",
      "Learning rate:  0.0001\n",
      " - 66s - loss: 0.0265 - acc: 0.9995 - val_loss: 0.6576 - val_acc: 0.9016\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.90304\n",
      "Epoch 96/100\n",
      "Learning rate:  0.0001\n",
      " - 66s - loss: 0.0248 - acc: 0.9996 - val_loss: 0.6628 - val_acc: 0.9013\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.90304\n",
      "Epoch 97/100\n",
      "Learning rate:  0.0001\n",
      " - 65s - loss: 0.0236 - acc: 0.9996 - val_loss: 0.6476 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.90304\n",
      "Epoch 98/100\n",
      "Learning rate:  0.0001\n",
      " - 66s - loss: 0.0223 - acc: 0.9996 - val_loss: 0.6609 - val_acc: 0.8997\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.90304\n",
      "Epoch 99/100\n",
      "Learning rate:  0.0001\n",
      " - 66s - loss: 0.0212 - acc: 0.9996 - val_loss: 0.6725 - val_acc: 0.9020\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.90304\n",
      "Epoch 100/100\n",
      "Learning rate:  0.0001\n",
      " - 66s - loss: 0.0204 - acc: 0.9995 - val_loss: 0.9647 - val_acc: 0.8577\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.90304\n",
      "Model:  resnet56v2_1_100_20 , Loss:  1.0172848474614757 , Accuracy:  0.8508680493195503\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read directory\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras import backend as k\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "model_name = 'resnet56v2_1_100_20'\n",
    "input_dir = '/content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/PKL_20v4/'\n",
    "output_dir = '/content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/'\n",
    "\n",
    "if os.path.isdir(output_dir):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "if os.path.isdir(output_dir + str(model_name)):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(output_dir + str(model_name))\n",
    "   \n",
    "filenames = os.listdir(input_dir)\n",
    "for filename in filenames:\n",
    "    if 'image' in filename:\n",
    "        print('loading %s' % filename)\n",
    "        with open(input_dir + filename, 'rb') as f:\n",
    "            X = pickle.load(f)\n",
    "        print('loaded!')\n",
    "    else:\n",
    "        print('loading %s' % filename)\n",
    "        with open(input_dir + filename, 'rb') as f:\n",
    "            Y = pickle.load(f)\n",
    "        print('loaded!')\n",
    "\n",
    "X = np.array(X)\n",
    "X = X[:, :, :, np.newaxis]\n",
    "input_shape = X.shape[1:]\n",
    "Y = np.array(Y)\n",
    "num_classes = len(Counter(Y))\n",
    "Y = to_categorical(Y, num_classes)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=37, shuffle=True)\n",
    "print('# of images: %d' % len(X))\n",
    "print('Shape of images:', X.shape[1:])\n",
    "print('# of train set: %d' % len(X_train), '# of test set: %d' % len(X_test))\n",
    "del X, Y\n",
    "\n",
    "# Define Model\n",
    "version = 2\n",
    "n = 6\n",
    "depth = n * 9 + 2\n",
    "batch_size = 128\n",
    "epochs = 100 # 200\n",
    "model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    conv = Conv2D(filters=filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v2(input_shape=input_shape, depth=depth, classes=num_classes):\n",
    "    filters_in = 16\n",
    "    resnet_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    i = Input(shape=input_shape, name='Input_Image')\n",
    "    x = resnet_layer(inputs=i, filters=filters_in, conv_first=True)\n",
    "    for stage in range(3):\n",
    "        for resnet_block in range(resnet_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                filters_out = filters_in * 4\n",
    "                if resnet_block == 0:\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                filters_out = filters_in * 2\n",
    "                if resnet_block == 0:\n",
    "                    strides = 2\n",
    "            y = resnet_layer(inputs=x, filters=filters_in, kernel_size=1, strides=strides, activation=activation, batch_normalization=batch_normalization, conv_first=False)\n",
    "            y = resnet_layer(inputs=y, filters=filters_in, conv_first=False)\n",
    "            y = resnet_layer(inputs=y, filters=filters_out, kernel_size=1, conv_first=False)\n",
    "            if resnet_block == 0:\n",
    "                x = resnet_layer(inputs=x, filters=filters_out, kernel_size=1, strides=strides, activation=None, batch_normalization=False)\n",
    "            x = layers.add([x, y])\n",
    "        filters_in = filters_out\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    # x = AveragePooling2D(pool_size=8)(x)\n",
    "    x = Flatten()(x)\n",
    "    y = Dense(classes, activation='softmax', kernel_initializer='he_normal')(x)\n",
    "    model = Model(inputs=i, outputs=y)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr_schedule(0)), metrics=['accuracy'])\n",
    "print(model_type)\n",
    "checkpoint = ModelCheckpoint(filepath=output_dir + str(model_name) + '/checkpoint.{epoch:03d}.hg',\n",
    "                             monitor='val_acc',\n",
    "                             verbose=2,\n",
    "                             save_best_only=True)\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "fit = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=2, callbacks=callbacks, validation_split=0.2, initial_epoch=0)\n",
    "eva = model.evaluate(X_test, Y_test, verbose=2)\n",
    "\n",
    "\n",
    "def plot_loss(history, output_dir, model_name):\n",
    "    plt.clf()\n",
    "    plt_loss = plt\n",
    "    plt_loss.plot(history.history['loss'])\n",
    "    plt_loss.plot(history.history['val_loss'])\n",
    "    plt_loss.title('Model Loss')\n",
    "    plt_loss.xlabel('Epoch')\n",
    "    plt_loss.ylabel('Loss')\n",
    "    plt_loss.legend(['Train', 'Test'], loc=0)\n",
    "    figure = output_dir + str(model_name) + '/loss.png'\n",
    "    plt_loss.savefig(figure, dpi=1080)\n",
    "\n",
    "\n",
    "def plot_acc(history, output_dir, model_name):\n",
    "    plt.clf()\n",
    "    plt_acc = plt\n",
    "    plt_acc.plot(history.history['acc'])\n",
    "    plt_acc.plot(history.history['val_acc'])\n",
    "    plt_acc.title('Model Accuracy')\n",
    "    plt_acc.xlabel('Epoch')\n",
    "    plt_acc.ylabel('Accuracy')\n",
    "    plt_acc.legend(['Train', 'Test'], loc=0)\n",
    "    figure = output_dir + str(model_name) + '/accuracy.png'\n",
    "    plt_acc.savefig(figure, dpi=1080)\n",
    "\n",
    "\n",
    "def csv_fit(history, output_dir, model_name):\n",
    "    train_data = pd.DataFrame(history.history)\n",
    "    train_data.to_csv(output_dir + str(model_name) + '/csv_fit.csv')\n",
    "    return None\n",
    "\n",
    "\n",
    "def csv_eva(history, output_dir, model_name):\n",
    "    test_data = pd.DataFrame(history)\n",
    "    test_data = test_data.T\n",
    "    if len(history) == 5:\n",
    "        test_data_header = ['test_loss', 'test_acc', 'precision', 'recall', 'f1score']\n",
    "    else:\n",
    "        test_data_header = ['test_loss', 'test_acc']\n",
    "    test_data.to_csv(output_dir + str(model_name) + '/csv_eva.csv', header=test_data_header)\n",
    "    return None\n",
    "\n",
    "\n",
    "print('Model: ', model_name, ', Loss: ', eva[0], ', Accuracy: ', eva[1])\n",
    "plot_loss(history=fit, output_dir=output_dir, model_name=model_name)\n",
    "plot_acc(history=fit, output_dir=output_dir, model_name=model_name)\n",
    "csv_fit(history=fit, output_dir=output_dir, model_name=model_name)\n",
    "csv_eva(history=eva, output_dir=output_dir, model_name=model_name)\n",
    "model.save(output_dir + str(model_name) + '/model.h5')\n",
    "model_json = model.to_json()\n",
    "with open(output_dir + str(model_name) + '/model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "    json_file.close()\n",
    "model_yaml = model.to_yaml()\n",
    "with open(output_dir + str(model_name) + '/model.yaml', 'w') as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "    yaml_file.close()\n",
    "model.save_weights(output_dir + str(model_name) + '/weights.h5')\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "P03jIzPmZt7x",
    "colab_type": "code",
    "outputId": "af0a0578-20a0-42d7-abd0-f1c3baf6f0e2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.560497865179E12,
     "user_tz": -540.0,
     "elapsed": 15498.0,
     "user": {
      "displayName": "Inyong Hwang",
      "photoUrl": "https://lh5.googleusercontent.com/-R8iA5nuNKY0/AAAAAAAAAAI/AAAAAAAAAAc/q2RGMSgONJg/s64/photo.jpg",
      "userId": "17775686403241352501"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====output=====\n",
      "accuracy.png\t   checkpoint.002.hg  csv_eva.csv  loss.png  weights.h5\n",
      "checkpoint.001.hg  checkpoint.006.hg  csv_fit.csv  model.h5\n"
     ]
    }
   ],
   "source": [
    "print('=====output=====')\n",
    "!ls '/content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_2/resnet56v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "tlTPcCPG-lDX",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "COLAB_2_ResNet_1_100_20.ipynb",
   "version": "0.3.2",
   "provenance": [
    {
     "file_id": "1vY3ME9i45zPbLGtP2r9XCDST--n0BkAP",
     "timestamp": 1.560693784606E12
    },
    {
     "file_id": "1iae9TyBzO1_n98a9eM4PRFBMSk7Obqg0",
     "timestamp": 1.560693183518E12
    },
    {
     "file_id": "1YFhVY6ZVnKuvtjN7A8vFhi_Gq-wuHNO5",
     "timestamp": 1.560493355521E12
    },
    {
     "file_id": "1oUBaAuby6umdZMtWxu5UTCkCihieEzko",
     "timestamp": 1.560479353623E12
    },
    {
     "file_id": "1OkE8SHoHTgYWYBgfaFvcyFHZyUhN4zOJ",
     "timestamp": 1.560308126915E12
    }
   ],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
