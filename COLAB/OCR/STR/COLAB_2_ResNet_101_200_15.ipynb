{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d3WhoYlxxc67",
    "colab_type": "code",
    "outputId": "d0a0b50b-ec08-49a9-c912-0abd0b3b08c5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.56070266373E12,
     "user_tz": -540.0,
     "elapsed": 920.0,
     "user": {
      "displayName": "L Kan",
      "photoUrl": "",
      "userId": "05643648821851919258"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Author: Inyong Hwang (lkan6004@gmail.com)\n",
    "# Date: 2019-06-17-Mon\n",
    "# Korean Character STR 15*15 by ResNet epoch=101~200 \n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SdC6ExhCxkKN",
    "colab_type": "code",
    "outputId": "5dd56327-04fb-4e1e-bfc0-c6314a12debe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.560704045551E12,
     "user_tz": -540.0,
     "elapsed": 4270.0,
     "user": {
      "displayName": "L Kan",
      "photoUrl": "",
      "userId": "05643648821851919258"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Google Drive=====\n",
      " 논문\t\t\t      'Colab Notebooks'   Lab-Desktop   PUBLIC\n",
      "'AI 사물인식 해커톤 (2).zip'   Dataset\t\t  Program       USB\n",
      "=====input=====\n",
      "images.pkl  labels.pkl\n"
     ]
    }
   ],
   "source": [
    "print('=====Google Drive=====')\n",
    "!ls '/content/drive/My Drive/'\n",
    "print('=====input=====')\n",
    "!ls '/content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/PKL_15v4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zajE3nISxsQA",
    "colab_type": "code",
    "outputId": "f71aa3d0-bdd7-4843-bc62-1a1dec7b81c6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.560709660259E12,
     "user_tz": -540.0,
     "elapsed": 2561465.0,
     "user": {
      "displayName": "L Kan",
      "photoUrl": "",
      "userId": "05643648821851919258"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17127.0
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading images.pkl\n",
      "loaded!\n",
      "loading labels.pkl\n",
      "loaded!\n",
      "# of images: 139104\n",
      "Shape of images: (15, 15, 1)\n",
      "# of train set: 111283 # of test set: 27821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0616 16:54:12.300040 140359683327872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0616 16:54:12.340658 140359683327872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0616 16:54:12.371261 140359683327872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0616 16:54:12.372121 140359683327872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0616 16:54:12.372997 140359683327872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0616 16:54:13.091904 140359683327872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0616 16:54:29.668276 140359683327872 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input_Image (InputLayer)        (None, 15, 15, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 15, 15, 16)   160         Input_Image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 15, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 15, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 15, 15, 16)   272         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 15, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 15, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 15, 15, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 15, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 15, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 15, 15, 64)   1088        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 15, 15, 64)   1088        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 15, 15, 64)   0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 15, 15, 64)   256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 15, 15, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 15, 15, 16)   1040        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 15, 15, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 15, 15, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 15, 15, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 15, 15, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 15, 15, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 15, 15, 64)   1088        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 15, 15, 64)   0           add_1[0][0]                      \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 15, 15, 64)   256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 15, 15, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 15, 15, 16)   1040        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 15, 15, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 15, 15, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 15, 15, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 15, 15, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 15, 15, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 15, 15, 64)   1088        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 15, 15, 64)   0           add_2[0][0]                      \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 15, 15, 64)   256         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 15, 15, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 15, 15, 16)   1040        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 15, 15, 16)   64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 15, 15, 16)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 15, 15, 16)   2320        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 15, 15, 16)   64          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 15, 15, 16)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 15, 15, 64)   1088        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 15, 15, 64)   0           add_3[0][0]                      \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 15, 15, 64)   256         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 15, 15, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 15, 15, 16)   1040        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 15, 15, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 15, 15, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 15, 15, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 15, 15, 16)   64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 15, 15, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 15, 15, 64)   1088        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 15, 15, 64)   0           add_4[0][0]                      \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 15, 15, 64)   256         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 15, 15, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 15, 15, 16)   1040        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 15, 15, 16)   64          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 15, 15, 16)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 15, 15, 16)   2320        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 15, 15, 16)   64          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 15, 15, 16)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 15, 15, 64)   1088        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 15, 15, 64)   0           add_5[0][0]                      \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 15, 15, 64)   256         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 15, 15, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 64)     4160        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 64)     256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 64)     0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 64)     36928       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 64)     256         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 64)     0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 8, 128)    8320        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 8, 8, 128)    8320        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 128)    0           conv2d_24[0][0]                  \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 128)    512         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 128)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 8, 8, 64)     8256        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 64)     256         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 64)     0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 64)     36928       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 64)     256         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 64)     0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 128)    8320        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 128)    0           add_7[0][0]                      \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 128)    512         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 128)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 64)     8256        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 64)     256         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 64)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 64)     36928       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 64)     256         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 64)     0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 128)    8320        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 128)    0           add_8[0][0]                      \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 128)    512         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 128)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 64)     8256        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 8, 64)     256         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 64)     0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 8, 64)     36928       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 64)     256         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 64)     0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 8, 128)    8320        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 8, 8, 128)    0           add_9[0][0]                      \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 8, 128)    512         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 128)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 8, 8, 64)     8256        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 8, 8, 64)     256         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 8, 64)     0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 8, 64)     36928       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 8, 8, 64)     256         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 8, 64)     0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 128)    8320        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 8, 128)    0           add_10[0][0]                     \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 8, 8, 128)    512         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 128)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 64)     8256        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 64)     256         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 64)     0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 64)     36928       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 8, 64)     256         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 64)     0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 128)    8320        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 8, 8, 128)    0           add_11[0][0]                     \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 8, 128)    512         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 128)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 4, 4, 128)    16512       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 4, 4, 128)    512         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 4, 4, 128)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 4, 4, 128)    147584      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 4, 4, 128)    512         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 4, 4, 128)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 4, 4, 256)    33024       add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 4, 4, 256)    33024       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 4, 4, 256)    0           conv2d_43[0][0]                  \n",
      "                                                                 conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 4, 4, 256)    1024        add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 4, 4, 256)    0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 4, 4, 128)    32896       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 4, 4, 128)    512         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 4, 4, 128)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 4, 4, 128)    147584      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 4, 4, 128)    512         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 4, 4, 128)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 4, 4, 256)    33024       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 4, 4, 256)    0           add_13[0][0]                     \n",
      "                                                                 conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 4, 4, 256)    1024        add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 256)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 4, 4, 128)    32896       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 4, 4, 128)    512         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 4, 4, 128)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 4, 128)    147584      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 4, 4, 128)    512         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 4, 4, 128)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 4, 4, 256)    33024       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 4, 4, 256)    0           add_14[0][0]                     \n",
      "                                                                 conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 4, 4, 256)    1024        add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 4, 4, 256)    0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 4, 4, 128)    32896       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 4, 4, 128)    512         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 4, 4, 128)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 4, 4, 128)    147584      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 4, 4, 128)    512         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 4, 128)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 4, 4, 256)    33024       activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 4, 4, 256)    0           add_15[0][0]                     \n",
      "                                                                 conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 4, 4, 256)    1024        add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 256)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 4, 4, 128)    32896       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 4, 4, 128)    512         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 4, 4, 128)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 4, 4, 128)    147584      activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 4, 4, 128)    512         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 4, 4, 128)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 4, 4, 256)    33024       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 4, 4, 256)    0           add_16[0][0]                     \n",
      "                                                                 conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 4, 4, 256)    1024        add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 4, 4, 256)    0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 4, 4, 128)    32896       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 4, 4, 128)    512         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 4, 4, 128)    0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 4, 4, 128)    147584      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 4, 4, 128)    512         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 4, 4, 128)    0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 4, 4, 256)    33024       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 4, 4, 256)    0           add_17[0][0]                     \n",
      "                                                                 conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 4, 4, 256)    1024        add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 4, 4, 256)    0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1008)         4129776     flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,800,656\n",
      "Trainable params: 5,790,256\n",
      "Non-trainable params: 10,400\n",
      "__________________________________________________________________________________________________\n",
      "Learning rate:  0.001\n",
      "ResNet56v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0616 16:54:30.544167 140359683327872 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 89026 samples, validate on 22257 samples\n",
      "Epoch 101/200\n",
      "Learning rate:  0.0001\n",
      " - 72s - loss: 0.0691 - acc: 0.9965 - val_loss: 2.0994 - val_acc: 0.7325\n",
      "\n",
      "Epoch 00101: val_acc improved from -inf to 0.73249, saving model to /content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_101_200_15/checkpoint.101.hg\n",
      "Epoch 102/200\n",
      "Learning rate:  0.0001\n",
      " - 56s - loss: 0.0671 - acc: 0.9964 - val_loss: 2.1521 - val_acc: 0.7318\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.73249\n",
      "Epoch 103/200\n",
      "Learning rate:  0.0001\n",
      " - 56s - loss: 0.0649 - acc: 0.9968 - val_loss: 2.1631 - val_acc: 0.7286\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.73249\n",
      "Epoch 104/200\n",
      "Learning rate:  0.0001\n",
      " - 55s - loss: 0.0636 - acc: 0.9969 - val_loss: 2.0809 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.73249\n",
      "Epoch 105/200\n",
      "Learning rate:  0.0001\n",
      " - 56s - loss: 0.0621 - acc: 0.9967 - val_loss: 2.0909 - val_acc: 0.7306\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.73249\n",
      "Epoch 106/200\n",
      "Learning rate:  0.0001\n",
      " - 55s - loss: 0.0609 - acc: 0.9967 - val_loss: 2.1378 - val_acc: 0.7291\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.73249\n",
      "Epoch 107/200\n",
      "Learning rate:  0.0001\n",
      " - 55s - loss: 0.0596 - acc: 0.9966 - val_loss: 2.1874 - val_acc: 0.7206\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.73249\n",
      "Epoch 108/200\n",
      "Learning rate:  0.0001\n",
      " - 55s - loss: 0.0578 - acc: 0.9969 - val_loss: 2.2688 - val_acc: 0.7194\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.73249\n",
      "Epoch 109/200\n",
      "Learning rate:  0.0001\n",
      " - 54s - loss: 0.0570 - acc: 0.9967 - val_loss: 2.1358 - val_acc: 0.7293\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.73249\n",
      "Epoch 110/200\n",
      "Learning rate:  0.0001\n",
      " - 55s - loss: 0.0562 - acc: 0.9966 - val_loss: 2.1482 - val_acc: 0.7255\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.73249\n",
      "Epoch 111/200\n",
      "Learning rate:  0.0001\n",
      " - 55s - loss: 0.0549 - acc: 0.9967 - val_loss: 2.1259 - val_acc: 0.7285\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.73249\n",
      "Epoch 112/200\n",
      "Learning rate:  0.0001\n",
      " - 55s - loss: 0.0534 - acc: 0.9969 - val_loss: 2.1769 - val_acc: 0.7266\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.73249\n",
      "Epoch 113/200\n",
      "Learning rate:  0.0001\n",
      " - 55s - loss: 0.0527 - acc: 0.9970 - val_loss: 2.1740 - val_acc: 0.7259\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.73249\n",
      "Epoch 114/200\n",
      "Learning rate:  0.0001\n",
      " - 55s - loss: 0.0517 - acc: 0.9970 - val_loss: 2.1921 - val_acc: 0.7274\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.73249\n",
      "Epoch 115/200\n",
      "Learning rate:  0.0001\n",
      " - 55s - loss: 0.0508 - acc: 0.9970 - val_loss: 2.2372 - val_acc: 0.7233\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.73249\n",
      "Epoch 116/200\n",
      "Learning rate:  0.0001\n",
      " - 55s - loss: 0.0499 - acc: 0.9969 - val_loss: 2.1773 - val_acc: 0.7202\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.73249\n",
      "Epoch 117/200\n",
      "Learning rate:  0.0001\n",
      " - 55s - loss: 0.0491 - acc: 0.9968 - val_loss: 2.1614 - val_acc: 0.7266\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.73249\n",
      "Epoch 118/200\n",
      "Learning rate:  0.0001\n",
      " - 55s - loss: 0.0484 - acc: 0.9969 - val_loss: 2.4646 - val_acc: 0.6945\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.73249\n",
      "Epoch 119/200\n",
      "Learning rate:  0.0001\n",
      " - 54s - loss: 0.0469 - acc: 0.9971 - val_loss: 2.2566 - val_acc: 0.7191\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.73249\n",
      "Epoch 120/200\n",
      "Learning rate:  0.0001\n",
      " - 55s - loss: 0.0469 - acc: 0.9969 - val_loss: 2.1547 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.73249\n",
      "Epoch 121/200\n",
      "Learning rate:  0.0001\n",
      " - 55s - loss: 0.0460 - acc: 0.9969 - val_loss: 2.2194 - val_acc: 0.7229\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.73249\n",
      "Epoch 122/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0435 - acc: 0.9982 - val_loss: 2.1896 - val_acc: 0.7257\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.73249\n",
      "Epoch 123/200\n",
      "Learning rate:  1e-05\n",
      " - 55s - loss: 0.0429 - acc: 0.9984 - val_loss: 2.1952 - val_acc: 0.7269\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.73249\n",
      "Epoch 124/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0425 - acc: 0.9984 - val_loss: 2.1943 - val_acc: 0.7274\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.73249\n",
      "Epoch 125/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0422 - acc: 0.9987 - val_loss: 2.1941 - val_acc: 0.7282\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.73249\n",
      "Epoch 126/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0419 - acc: 0.9987 - val_loss: 2.2088 - val_acc: 0.7279\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.73249\n",
      "Epoch 127/200\n",
      "Learning rate:  1e-05\n",
      " - 55s - loss: 0.0418 - acc: 0.9988 - val_loss: 2.2071 - val_acc: 0.7282\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.73249\n",
      "Epoch 128/200\n",
      "Learning rate:  1e-05\n",
      " - 55s - loss: 0.0415 - acc: 0.9988 - val_loss: 2.2114 - val_acc: 0.7280\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.73249\n",
      "Epoch 129/200\n",
      "Learning rate:  1e-05\n",
      " - 55s - loss: 0.0414 - acc: 0.9987 - val_loss: 2.2089 - val_acc: 0.7288\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.73249\n",
      "Epoch 130/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0412 - acc: 0.9989 - val_loss: 2.2110 - val_acc: 0.7286\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.73249\n",
      "Epoch 131/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0410 - acc: 0.9987 - val_loss: 2.2161 - val_acc: 0.7289\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.73249\n",
      "Epoch 132/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0409 - acc: 0.9986 - val_loss: 2.2157 - val_acc: 0.7285\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.73249\n",
      "Epoch 133/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0407 - acc: 0.9988 - val_loss: 2.2162 - val_acc: 0.7295\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.73249\n",
      "Epoch 134/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0405 - acc: 0.9988 - val_loss: 2.2219 - val_acc: 0.7285\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.73249\n",
      "Epoch 135/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0403 - acc: 0.9988 - val_loss: 2.2223 - val_acc: 0.7288\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.73249\n",
      "Epoch 136/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0402 - acc: 0.9988 - val_loss: 2.2298 - val_acc: 0.7288\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.73249\n",
      "Epoch 137/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0400 - acc: 0.9987 - val_loss: 2.2203 - val_acc: 0.7286\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.73249\n",
      "Epoch 138/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0398 - acc: 0.9989 - val_loss: 2.2312 - val_acc: 0.7281\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.73249\n",
      "Epoch 139/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0397 - acc: 0.9989 - val_loss: 2.2307 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.73249\n",
      "Epoch 140/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0396 - acc: 0.9988 - val_loss: 2.2266 - val_acc: 0.7284\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.73249\n",
      "Epoch 141/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0394 - acc: 0.9989 - val_loss: 2.2304 - val_acc: 0.7287\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.73249\n",
      "Epoch 142/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0393 - acc: 0.9988 - val_loss: 2.2341 - val_acc: 0.7285\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.73249\n",
      "Epoch 143/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0391 - acc: 0.9988 - val_loss: 2.2318 - val_acc: 0.7289\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.73249\n",
      "Epoch 144/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0390 - acc: 0.9989 - val_loss: 2.2396 - val_acc: 0.7288\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.73249\n",
      "Epoch 145/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0388 - acc: 0.9989 - val_loss: 2.2375 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.73249\n",
      "Epoch 146/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0387 - acc: 0.9990 - val_loss: 2.2405 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.73249\n",
      "Epoch 147/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0386 - acc: 0.9989 - val_loss: 2.2386 - val_acc: 0.7282\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.73249\n",
      "Epoch 148/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0385 - acc: 0.9989 - val_loss: 2.2411 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.73249\n",
      "Epoch 149/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0383 - acc: 0.9989 - val_loss: 2.2465 - val_acc: 0.7278\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.73249\n",
      "Epoch 150/200\n",
      "Learning rate:  1e-05\n",
      " - 54s - loss: 0.0382 - acc: 0.9989 - val_loss: 2.2454 - val_acc: 0.7275\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.73249\n",
      "Epoch 151/200\n",
      "Learning rate:  1e-05\n",
      " - 55s - loss: 0.0380 - acc: 0.9989 - val_loss: 2.2453 - val_acc: 0.7279\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.73249\n",
      "Epoch 152/200\n",
      "Learning rate:  1e-05\n",
      " - 56s - loss: 0.0379 - acc: 0.9989 - val_loss: 2.2406 - val_acc: 0.7275\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.73249\n",
      "Epoch 153/200\n",
      "Learning rate:  1e-05\n",
      " - 55s - loss: 0.0378 - acc: 0.9989 - val_loss: 2.2431 - val_acc: 0.7279\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.73249\n",
      "Epoch 154/200\n",
      "Learning rate:  1e-05\n",
      " - 55s - loss: 0.0377 - acc: 0.9989 - val_loss: 2.2482 - val_acc: 0.7277\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.73249\n",
      "Epoch 155/200\n",
      "Learning rate:  1e-05\n",
      " - 56s - loss: 0.0375 - acc: 0.9990 - val_loss: 2.2497 - val_acc: 0.7274\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.73249\n",
      "Epoch 156/200\n",
      "Learning rate:  1e-05\n",
      " - 55s - loss: 0.0374 - acc: 0.9989 - val_loss: 2.2439 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.73249\n",
      "Epoch 157/200\n",
      "Learning rate:  1e-05\n",
      " - 56s - loss: 0.0373 - acc: 0.9990 - val_loss: 2.2564 - val_acc: 0.7266\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.73249\n",
      "Epoch 158/200\n",
      "Learning rate:  1e-05\n",
      " - 55s - loss: 0.0371 - acc: 0.9991 - val_loss: 2.2561 - val_acc: 0.7263\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.73249\n",
      "Epoch 159/200\n",
      "Learning rate:  1e-05\n",
      " - 55s - loss: 0.0370 - acc: 0.9990 - val_loss: 2.2504 - val_acc: 0.7269\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.73249\n",
      "Epoch 160/200\n",
      "Learning rate:  1e-05\n",
      " - 56s - loss: 0.0370 - acc: 0.9990 - val_loss: 2.2523 - val_acc: 0.7265\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.73249\n",
      "Epoch 161/200\n",
      "Learning rate:  1e-05\n",
      " - 56s - loss: 0.0368 - acc: 0.9990 - val_loss: 2.2591 - val_acc: 0.7267\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.73249\n",
      "Epoch 162/200\n",
      "Learning rate:  1e-06\n",
      " - 56s - loss: 0.0365 - acc: 0.9993 - val_loss: 2.2509 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.73249\n",
      "Epoch 163/200\n",
      "Learning rate:  1e-06\n",
      " - 56s - loss: 0.0364 - acc: 0.9994 - val_loss: 2.2534 - val_acc: 0.7271\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.73249\n",
      "Epoch 164/200\n",
      "Learning rate:  1e-06\n",
      " - 56s - loss: 0.0364 - acc: 0.9994 - val_loss: 2.2543 - val_acc: 0.7271\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.73249\n",
      "Epoch 165/200\n",
      "Learning rate:  1e-06\n",
      " - 56s - loss: 0.0364 - acc: 0.9994 - val_loss: 2.2516 - val_acc: 0.7271\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.73249\n",
      "Epoch 166/200\n",
      "Learning rate:  1e-06\n",
      " - 56s - loss: 0.0363 - acc: 0.9995 - val_loss: 2.2539 - val_acc: 0.7271\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.73249\n",
      "Epoch 167/200\n",
      "Learning rate:  1e-06\n",
      " - 56s - loss: 0.0364 - acc: 0.9994 - val_loss: 2.2541 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.73249\n",
      "Epoch 168/200\n",
      "Learning rate:  1e-06\n",
      " - 56s - loss: 0.0364 - acc: 0.9995 - val_loss: 2.2558 - val_acc: 0.7271\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.73249\n",
      "Epoch 169/200\n",
      "Learning rate:  1e-06\n",
      " - 57s - loss: 0.0363 - acc: 0.9995 - val_loss: 2.2549 - val_acc: 0.7271\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.73249\n",
      "Epoch 170/200\n",
      "Learning rate:  1e-06\n",
      " - 57s - loss: 0.0363 - acc: 0.9995 - val_loss: 2.2568 - val_acc: 0.7272\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.73249\n",
      "Epoch 171/200\n",
      "Learning rate:  1e-06\n",
      " - 58s - loss: 0.0363 - acc: 0.9995 - val_loss: 2.2568 - val_acc: 0.7272\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.73249\n",
      "Epoch 172/200\n",
      "Learning rate:  1e-06\n",
      " - 57s - loss: 0.0363 - acc: 0.9994 - val_loss: 2.2561 - val_acc: 0.7266\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.73249\n",
      "Epoch 173/200\n",
      "Learning rate:  1e-06\n",
      " - 57s - loss: 0.0363 - acc: 0.9995 - val_loss: 2.2578 - val_acc: 0.7272\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.73249\n",
      "Epoch 174/200\n",
      "Learning rate:  1e-06\n",
      " - 57s - loss: 0.0363 - acc: 0.9995 - val_loss: 2.2572 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.73249\n",
      "Epoch 175/200\n",
      "Learning rate:  1e-06\n",
      " - 57s - loss: 0.0362 - acc: 0.9995 - val_loss: 2.2569 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.73249\n",
      "Epoch 176/200\n",
      "Learning rate:  1e-06\n",
      " - 55s - loss: 0.0362 - acc: 0.9994 - val_loss: 2.2562 - val_acc: 0.7271\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.73249\n",
      "Epoch 177/200\n",
      "Learning rate:  1e-06\n",
      " - 54s - loss: 0.0362 - acc: 0.9995 - val_loss: 2.2565 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.73249\n",
      "Epoch 178/200\n",
      "Learning rate:  1e-06\n",
      " - 54s - loss: 0.0362 - acc: 0.9994 - val_loss: 2.2585 - val_acc: 0.7271\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.73249\n",
      "Epoch 179/200\n",
      "Learning rate:  1e-06\n",
      " - 55s - loss: 0.0362 - acc: 0.9995 - val_loss: 2.2588 - val_acc: 0.7271\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.73249\n",
      "Epoch 180/200\n",
      "Learning rate:  1e-06\n",
      " - 56s - loss: 0.0362 - acc: 0.9995 - val_loss: 2.2561 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.73249\n",
      "Epoch 181/200\n",
      "Learning rate:  1e-06\n",
      " - 55s - loss: 0.0362 - acc: 0.9995 - val_loss: 2.2583 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.73249\n",
      "Epoch 182/200\n",
      "Learning rate:  5e-07\n",
      " - 54s - loss: 0.0361 - acc: 0.9994 - val_loss: 2.2553 - val_acc: 0.7271\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.73249\n",
      "Epoch 183/200\n",
      "Learning rate:  5e-07\n",
      " - 54s - loss: 0.0361 - acc: 0.9996 - val_loss: 2.2587 - val_acc: 0.7271\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.73249\n",
      "Epoch 184/200\n",
      "Learning rate:  5e-07\n",
      " - 54s - loss: 0.0361 - acc: 0.9996 - val_loss: 2.2583 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.73249\n",
      "Epoch 185/200\n",
      "Learning rate:  5e-07\n",
      " - 55s - loss: 0.0361 - acc: 0.9995 - val_loss: 2.2580 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.73249\n",
      "Epoch 186/200\n",
      "Learning rate:  5e-07\n",
      " - 55s - loss: 0.0361 - acc: 0.9994 - val_loss: 2.2597 - val_acc: 0.7269\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.73249\n",
      "Epoch 187/200\n",
      "Learning rate:  5e-07\n",
      " - 55s - loss: 0.0361 - acc: 0.9995 - val_loss: 2.2561 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.73249\n",
      "Epoch 188/200\n",
      "Learning rate:  5e-07\n",
      " - 56s - loss: 0.0361 - acc: 0.9994 - val_loss: 2.2596 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.73249\n",
      "Epoch 189/200\n",
      "Learning rate:  5e-07\n",
      " - 56s - loss: 0.0361 - acc: 0.9995 - val_loss: 2.2605 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.73249\n",
      "Epoch 190/200\n",
      "Learning rate:  5e-07\n",
      " - 56s - loss: 0.0361 - acc: 0.9996 - val_loss: 2.2580 - val_acc: 0.7272\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.73249\n",
      "Epoch 191/200\n",
      "Learning rate:  5e-07\n",
      " - 56s - loss: 0.0361 - acc: 0.9996 - val_loss: 2.2590 - val_acc: 0.7268\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.73249\n",
      "Epoch 192/200\n",
      "Learning rate:  5e-07\n",
      " - 56s - loss: 0.0361 - acc: 0.9996 - val_loss: 2.2607 - val_acc: 0.7266\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.73249\n",
      "Epoch 193/200\n",
      "Learning rate:  5e-07\n",
      " - 56s - loss: 0.0361 - acc: 0.9996 - val_loss: 2.2597 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.73249\n",
      "Epoch 194/200\n",
      "Learning rate:  5e-07\n",
      " - 56s - loss: 0.0361 - acc: 0.9994 - val_loss: 2.2588 - val_acc: 0.7266\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.73249\n",
      "Epoch 195/200\n",
      "Learning rate:  5e-07\n",
      " - 56s - loss: 0.0360 - acc: 0.9996 - val_loss: 2.2589 - val_acc: 0.7266\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.73249\n",
      "Epoch 196/200\n",
      "Learning rate:  5e-07\n",
      " - 56s - loss: 0.0360 - acc: 0.9994 - val_loss: 2.2593 - val_acc: 0.7267\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.73249\n",
      "Epoch 197/200\n",
      "Learning rate:  5e-07\n",
      " - 57s - loss: 0.0360 - acc: 0.9995 - val_loss: 2.2603 - val_acc: 0.7270\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.73249\n",
      "Epoch 198/200\n",
      "Learning rate:  5e-07\n",
      " - 56s - loss: 0.0360 - acc: 0.9995 - val_loss: 2.2616 - val_acc: 0.7272\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.73249\n",
      "Epoch 199/200\n",
      "Learning rate:  5e-07\n",
      " - 57s - loss: 0.0360 - acc: 0.9994 - val_loss: 2.2605 - val_acc: 0.7269\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.73249\n",
      "Epoch 200/200\n",
      "Learning rate:  5e-07\n",
      " - 56s - loss: 0.0360 - acc: 0.9996 - val_loss: 2.2612 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.73249\n",
      "Model:  resnet56v2_101_200_15 , Loss:  2.256267579701077 , Accuracy:  0.7297365299636681\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read directory\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras import backend as k\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.models import model_from_json\n",
    "\n",
    "model_name = 'resnet56v2_101_200_15'\n",
    "input_dir = '/content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/PKL_15v4/'\n",
    "output_dir = '/content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/'\n",
    "json_dir = '/content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_15/model.json'\n",
    "weights_dir = '/content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_6/resnet56v2_1_100_15/weights.h5'\n",
    "\n",
    "if os.path.isdir(output_dir):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "if os.path.isdir(output_dir + str(model_name)):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(output_dir + str(model_name))\n",
    "   \n",
    "filenames = os.listdir(input_dir)\n",
    "for filename in filenames:\n",
    "    if 'image' in filename:\n",
    "        print('loading %s' % filename)\n",
    "        with open(input_dir + filename, 'rb') as f:\n",
    "            X = pickle.load(f)\n",
    "        print('loaded!')\n",
    "    else:\n",
    "        print('loading %s' % filename)\n",
    "        with open(input_dir + filename, 'rb') as f:\n",
    "            Y = pickle.load(f)\n",
    "        print('loaded!')\n",
    "\n",
    "X = np.array(X)\n",
    "X = X[:, :, :, np.newaxis]\n",
    "input_shape = X.shape[1:]\n",
    "Y = np.array(Y)\n",
    "num_classes = len(Counter(Y))\n",
    "Y = to_categorical(Y, num_classes)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=37, shuffle=True)\n",
    "print('# of images: %d' % len(X))\n",
    "print('Shape of images:', X.shape[1:])\n",
    "print('# of train set: %d' % len(X_train), '# of test set: %d' % len(X_test))\n",
    "del X, Y\n",
    "\n",
    "# Define \n",
    "\n",
    "version = 2\n",
    "n = 6\n",
    "depth = n * 9 + 2\n",
    "batch_size = 128\n",
    "epochs = 200 # 200\n",
    "model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "json_file = open(json_dir, 'r')\n",
    "json_model = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(json_model)\n",
    "model.load_weights(weights_dir)\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr_schedule(0)), metrics=['accuracy'])\n",
    "print(model_type)\n",
    "checkpoint = ModelCheckpoint(filepath=output_dir + str(model_name) + '/checkpoint.{epoch:03d}.hg',\n",
    "                             monitor='val_acc',\n",
    "                             verbose=2,\n",
    "                             save_best_only=True)\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "fit = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=2, callbacks=callbacks, validation_split=0.2, initial_epoch=100)\n",
    "eva = model.evaluate(X_test, Y_test, verbose=2)\n",
    "\n",
    "\n",
    "def plot_loss(history, output_dir, model_name):\n",
    "    plt.clf()\n",
    "    plt_loss = plt\n",
    "    plt_loss.plot(history.history['loss'])\n",
    "    plt_loss.plot(history.history['val_loss'])\n",
    "    plt_loss.title('Model Loss')\n",
    "    plt_loss.xlabel('Epoch')\n",
    "    plt_loss.ylabel('Loss')\n",
    "    plt_loss.legend(['Train', 'Test'], loc=0)\n",
    "    figure = output_dir + str(model_name) + '/loss.png'\n",
    "    plt_loss.savefig(figure, dpi=1080)\n",
    "\n",
    "\n",
    "def plot_acc(history, output_dir, model_name):\n",
    "    plt.clf()\n",
    "    plt_acc = plt\n",
    "    plt_acc.plot(history.history['acc'])\n",
    "    plt_acc.plot(history.history['val_acc'])\n",
    "    plt_acc.title('Model Accuracy')\n",
    "    plt_acc.xlabel('Epoch')\n",
    "    plt_acc.ylabel('Accuracy')\n",
    "    plt_acc.legend(['Train', 'Test'], loc=0)\n",
    "    figure = output_dir + str(model_name) + '/accuracy.png'\n",
    "    plt_acc.savefig(figure, dpi=1080)\n",
    "\n",
    "\n",
    "def csv_fit(history, output_dir, model_name):\n",
    "    train_data = pd.DataFrame(history.history)\n",
    "    train_data.to_csv(output_dir + str(model_name) + '/csv_fit.csv')\n",
    "    return None\n",
    "\n",
    "\n",
    "def csv_eva(history, output_dir, model_name):\n",
    "    test_data = pd.DataFrame(history)\n",
    "    test_data = test_data.T\n",
    "    if len(history) == 5:\n",
    "        test_data_header = ['test_loss', 'test_acc', 'precision', 'recall', 'f1score']\n",
    "    else:\n",
    "        test_data_header = ['test_loss', 'test_acc']\n",
    "    test_data.to_csv(output_dir + str(model_name) + '/csv_eva.csv', header=test_data_header)\n",
    "    return None\n",
    "\n",
    "\n",
    "print('Model: ', model_name, ', Loss: ', eva[0], ', Accuracy: ', eva[1])\n",
    "plot_loss(history=fit, output_dir=output_dir, model_name=model_name)\n",
    "plot_acc(history=fit, output_dir=output_dir, model_name=model_name)\n",
    "csv_fit(history=fit, output_dir=output_dir, model_name=model_name)\n",
    "csv_eva(history=eva, output_dir=output_dir, model_name=model_name)\n",
    "model.save(output_dir + str(model_name) + '/model.h5')\n",
    "model_json = model.to_json()\n",
    "with open(output_dir + str(model_name) + '/model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "    json_file.close()\n",
    "model_yaml = model.to_yaml()\n",
    "with open(output_dir + str(model_name) + '/model.yaml', 'w') as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "    yaml_file.close()\n",
    "model.save_weights(output_dir + str(model_name) + '/weights.h5')\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "P03jIzPmZt7x",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "print('=====output=====')\n",
    "!ls '/content/drive/My Drive/PUBLIC/PROJECT/OCR/STR/COLAB_OUTPUT_2/resnet56v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "tlTPcCPG-lDX",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "COLAB_2_ResNet_101_200_15.ipynb",
   "version": "0.3.2",
   "provenance": [
    {
     "file_id": "1fhrVN5LXC5s5gCOg7BlL9JIoSA_2PS4l",
     "timestamp": 1.560689703886E12
    },
    {
     "file_id": "13-lxwu3qrotHXNoOJYlVQmXFiK1QF67H",
     "timestamp": 1.560590481871E12
    },
    {
     "file_id": "1Oznv-mZazv25aXZjXV6vfb0xphxR2C7o",
     "timestamp": 1.560564439854E12
    },
    {
     "file_id": "1HdIYR8Sf10if8VmuF34CNP5ArUQJ0-Zs",
     "timestamp": 1.560534966683E12
    },
    {
     "file_id": "1A6BwnddGlmqiw9_7RNdf3lbwQhlJVGGw",
     "timestamp": 1.560508194285E12
    },
    {
     "file_id": "1iae9TyBzO1_n98a9eM4PRFBMSk7Obqg0",
     "timestamp": 1.560497983028E12
    },
    {
     "file_id": "1YFhVY6ZVnKuvtjN7A8vFhi_Gq-wuHNO5",
     "timestamp": 1.560493355521E12
    },
    {
     "file_id": "1oUBaAuby6umdZMtWxu5UTCkCihieEzko",
     "timestamp": 1.560479353623E12
    },
    {
     "file_id": "1OkE8SHoHTgYWYBgfaFvcyFHZyUhN4zOJ",
     "timestamp": 1.560308126915E12
    }
   ],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
